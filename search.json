[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Brandon Lee",
    "section": "",
    "text": "I’m a huge believer of being a life time learner where the learning doesn’t stop once you’ve reached a certain career goal. No, I believe learning should be thought of as a tool that will allow you to become the best version of yourself. A tool that allows you to be constantly honed and sharpened, ready to face any challenges that face your way - be it personal or professional.\nWhen I’m not trying to preach my life philosophy to strangers online, I’m usually building, tinkering with, or thinking of new ideas for data science projects in my free time. These usually start with a small idea but can somehow grow to multiple fully-fledged projects. More often than not, they come about because I want to put some new skills or concepts I’ve learned on client projects into practice. Outside of data science, I’m also pretty passionate about physical fitness.\nHopefully you’ll learn a thing or two from visiting my blog, enjoy your stay and feel free to connect on LinkedIn or send me a message!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Solverism",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n  \n\n\n\n\n\n\n\n\nJoin me as I dig out insights from the Spotify metadata of my playlists.\n\n\n\n\n\n\nNov 7, 2022\n\n\n15 min\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nMeasuring food price inflation in a way that anyone in the UK can relate to.\n\n\n\n\n\n\nSep 20, 2022\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html",
    "href": "posts/2022-09-20-tesco-price-analysis.html",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "",
    "text": "Source: https://www.rte.ie/news/business/2022/0908/1321302-cso-inflation-figures/\nDisclaimer: The use of data collected for this study is covered by the Intellectual Property Office’s Exceptions to Copyright for Non-Commercial Research and Private Study. No personal information has been collected as part of this study’s data collection process and web scraping best practices implemented to the best of the author’s abilities. The collected data will be inaccessible to the public to abide by Section 7 of the Terms and Conditions agreement laid out by Tesco PLC.\nSince July 2021 the Consumer Prices Index (CPI) inflation rate in the UK has broken away from the Bank of England’s target rate of 2%, steadily rising month by month. As of 3rd September 2022, the inflation rate stands at 10.1% with no indication of decreasing. What an awfully depressing way to start a blog post but unfortunately this is the reality that we are all currently facing in the UK. Every month the Office for National Statistics (ONS) calculates CPI by collecting the prices for a basket of around 730 different consumer goods and services that is a representation of the society’s current buying habits and measures the average change in price of the basket on a year-over-year basis. At a risk of grossly simplifying, CPI tells us how much prices in general have changed over the last 12 months.\nThat’s all nice and well but society’s buying habits doesn’t necessarily reflect my buying habits. There are 12 broad categories of goods and services in the CPI basket, some of these categories such as clothing and footwear, and furniture and household goods don’t apply to me as I rarely ever buy new clothes or furniture. The full list of categories and goods and services of each category can be found on the ONS website here. One category that does apply to me, and to everyone else, is food and drinks. Food and drink goods (excluding alcohol) currently account for 9.3% of the weighting in CPI, which is a relatively small slice of the pie.\nDuring recent times I’ve noticed that my shopping bill has gone up overall but I can’t quite pin down what items are contributing the most to this increase. To this end, I’ve built a cloud-based web-scraping application that scrapes product data from Tesco’s website including price and categories (GitHub link - blog article incoming). The reason why I chose Tesco as opposed to any other supermarket is because it’s 5 minutes down the road from me.\nIn this article we’ll load, process, transform, and analyse this scraped data to see how the prices of food categories have changed over time. If you want to skip directly to the charts and analysis, head to the ‘Charts and Analysis’ section in the above table of contents."
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#overview-of-price-changes-for-all-categories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#overview-of-price-changes-for-all-categories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Overview of Price Changes for All Categories",
    "text": "Overview of Price Changes for All Categories\nLet’s start with the big picture (quite literally), how did prices change when comparing the basket values of categories?\n\n\n\n\n\n\nThis chart is interactive!\n\n\n\nHover your mouse over the sector to look at the index number of categories (if on mobile, tap and hold the name of the category). Clicking on a sector will focus on that particular category and its subcategories. There are 4 levels of categories but only 3 levels are shown at any one time for better performance.\n\n\n\n\n\n\n\n\nThe index numbers presented in the graph below are accurate when rounding to whole numbers!\n\n\n\n\n\nTechnical explanation - The numbers get more inaccurate as you go up category granularities due to the way that the index numbers are aggregated. I’m using the most granular dataset that contains index numbers of quaternary categories, these are then aggregated up to the tertiary category level using the harmonic mean weighted by the basket value of the quaternary categories within the tertiary category. This process repeats for the secondary and then the primary categories, the approximation error increasing going up with each category level.\n\n\n\n\n\n\n                                                \n\n\nStraight away we can see that fresh food and frozen food have increased by ~3% and ~4% respectively. The major contributing subcategories for fresh food are cheese (~5% increase); fresh meat and poultry (~5% increase); ready meals (~4% increase); and yogurts (~4% increase). For frozen food, the major contributing subcategories are frozen fish and seafood (~9% increase); frozen chips, onion rings, potatoes, and rice (~12% increase); and frozen meat and poultry (~5% increase).\nYikes, no wonder why my shopping bill has been going up. Fresh chicken breast and thighs have gone up by over 10%!\nOverall, prices have by 1.7% across all categories and products in the processed dataset.\nWe can also see there are a few secondary categories that have gone down, most notably is the cooking sauces, meal kits, and sides subcategory within the food cupboard category which has gone down by ~2%. This does raise another question though, how many categories in each level experienced an increase, decrease, or no change in price?\n\n\n\n\n\n\n\nIt’s very clear from the above that the bakery and frozen food primary categories had all secondary, tertiary, and quaternary subcategories either increase in price or stay the same! We’ll have to note that they’re the two smallest primary categories by both total basket value and number of subcategories, but there are ongoing macroeconomic events that are affecting both the supply chain and cost of businesses.\nAs of writing, the 2022 Russian invasion of Ukraine is affecting the two largest global suppliers of wheat. Russia produces 11% of the world’s wheat and accounts for 19% of global wheat exports. On the other hand, Ukraine produces 3% of the world’s wheat and accounts for 9% of global wheat exports. Both of these together account for more than a quarter of the world’s wheat export market.1 You can guess what ingredient bakery goods use that’s derived from wheat.1 Source: https://asmith.ucdavis.edu/news/russia-ukraine\nEnergy prices have also been rising throughout 2022 and are forecasted to reach unprecedented levels during the first two quarters of 2023. The electricity price (kWh) cap rose from 20.8p in Q1 2022 to 28.3p in Q2 2022,2 an increase of 36%. This will affect the production costs of all goods, more so if they need to be kept cold or frozen.2 Source: https://www.icaew.com/insights/viewpoints-on-the-news/2022/sept-2022/chart-of-the-week-energy-price-cap-update\nGenerally speaking, it does seem that the vast majority of subcategories have experienced an increase in price, no matter what level of category granularity is being looked at. The drinks and food cupboard primary categories are the least affected as they’re the only two that contain secondary categories that have decreased in price; that being said they only account for 11.1% and 18.8% of the total number of their respective total secondary categories."
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-primary-food-categories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-primary-food-categories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Primary Food Categories",
    "text": "Price Changes for Primary Food Categories\nLet’s now take a look at how the index of primary categories changed over time as opposed to a snapshot in time.\n\n\n\n\n\n\n\nThere are a few things I’m seeing from this graph. First is that bakery, drinks, and food cupboard items seem to be plateauing where as fresh and frozen items don’t seem to be slowing down. Second, the fresh and frozen items have the highest rate of increase amongst all primary categories. Third, the food cupboard and drinks categories seem to increase at the same rate were it not for the initial hike on 6th March. Finally, the bakery category is the only one that looks to practically stay the same for the first five weeks before rapidly increasing at the same rate as the fresh and frozen food categories and then plateauing.\nThe big question that comes to mind is what’s driving the price of fresh food items to go up? If we extrapolate out the trend for an entire year, we’re looking at an approximate increase of 12.5% for both categories! To answer this question we’ll have to dive into the secondary categories."
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-fresh-food-subcategories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-fresh-food-subcategories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Fresh Food Subcategories",
    "text": "Price Changes for Fresh Food Subcategories\n\n\n\n\n\n\n\n\nWow there’s a lot going on in that graph, but can definitely be seen that some secondary categories are increasing much more than others. Let’s take a look at the index values at the end of the graph to quickly look at which secondary categories increased the most.\n\n\n\n\n\n\n  \n    \n      \n      Week Commencing Monday\n      Primary Category\n      Secondary Category\n      Total Basket Value (£)\n      Index\n    \n  \n  \n    \n      0\n      2022-06-13\n      fresh-food\n      cheese\n      543.92\n      104.73\n    \n    \n      1\n      2022-06-13\n      fresh-food\n      fresh-meat-and-poultry\n      1311.26\n      104.72\n    \n    \n      2\n      2022-06-13\n      fresh-food\n      yoghurts\n      389.49\n      104.13\n    \n    \n      3\n      2022-06-13\n      fresh-food\n      cooked-meats-antipasti-and-dips\n      537.09\n      103.95\n    \n    \n      4\n      2022-06-13\n      fresh-food\n      chilled-fish-and-seafood\n      270.53\n      103.56\n    \n    \n      5\n      2022-06-13\n      fresh-food\n      milk-butter-and-eggs\n      324.88\n      103.14\n    \n    \n      6\n      2022-06-13\n      fresh-food\n      ready-meals\n      653.29\n      102.98\n    \n    \n      7\n      2022-06-13\n      fresh-food\n      pies-pasties-quiches-and-snacking\n      322.31\n      102.83\n    \n    \n      8\n      2022-06-13\n      fresh-food\n      fresh-salad-coleslaw-and-sandwich-fillers\n      152.96\n      102.35\n    \n    \n      9\n      2022-06-13\n      fresh-food\n      chilled-vegetarian-and-vegan\n      290.60\n      102.29\n    \n    \n      10\n      2022-06-13\n      fresh-food\n      chilled-soup-sandwiches-and-salad-pots\n      226.00\n      101.87\n    \n    \n      11\n      2022-06-13\n      fresh-food\n      dairy-free-and-dairy-alternatives\n      328.15\n      101.85\n    \n    \n      12\n      2022-06-13\n      fresh-food\n      chilled-desserts\n      275.90\n      101.80\n    \n    \n      13\n      2022-06-13\n      fresh-food\n      fresh-vegetables\n      164.09\n      101.74\n    \n    \n      14\n      2022-06-13\n      fresh-food\n      juice-and-smoothies\n      44.01\n      100.62\n    \n    \n      15\n      2022-06-13\n      fresh-food\n      fresh-pizza-pasta-and-garlic-bread\n      292.38\n      100.30\n    \n    \n      16\n      2022-06-13\n      fresh-food\n      counters\n      100.25\n      100.10\n    \n    \n      17\n      2022-06-13\n      fresh-food\n      fresh-fruit\n      239.01\n      100.10\n    \n  \n\n\n\n\nLooking at the top 6 rows, it’s primarily dairy and fresh meat and poultry that’s driving up the index of fresh food items. Both of these being products produced from animals.\nAccording to the Specialty Food Magazine, the price increase of dairy products can be largely attributed to the spiralling price of feed, fuel, and fertiliser.3 All of these three production costs have been subject to large price hikes. For feed and fertiliser, it can be attributed to the Russian invasion on Ukraine, higher energy costs, and increased global demand.4 For fuel, the primary driver is an increase in the refining cost of crude oil.53 Source: https://www.specialityfoodmagazine.com/news/dairy-crisis-threatens-spiralling-costs-of-cheese-and-milk-for-indies4 Source: https://lordslibrary.parliament.uk/rising-cost-of-agricultural-fertiliser-and-feed-causes-impacts-and-government-policy/5 Source: https://www.allstarcard.co.uk/news-insights/business/why-is-fuel-expensive/\nIn Specialty Food Magazine’s article they have a quote from the vice-chair of the National Union of Farmers that it takes him two and a half years to get a cow from being born to be actually producing milk which incurs a lot of cost over the time period. In order to maintain their profit margins, they must raise their prices."
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#correlation-heatmap-of-all-secondary-categories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#correlation-heatmap-of-all-secondary-categories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Correlation Heatmap of All Secondary Categories",
    "text": "Correlation Heatmap of All Secondary Categories\nNote that the below graph is using Pearson’s Correlation coefficient. When the index of two subcategories move in tandem (i.e. they’ve got a positive correlation) the rectange will show as red, the stronger the correlation the more red the rectangle will be. Similar logic applies for when two subcategories move in opposite directions, except with blue. Interesting observation that the adult soft drink and mixers subcategory (first row/column) isn’t showing much of a correlation with anything else."
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-fresh-meat-and-poultry-subcategories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-fresh-meat-and-poultry-subcategories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Fresh Meat and Poultry Subcategories",
    "text": "Price Changes for Fresh Meat and Poultry Subcategories"
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-cheese-subcategories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-cheese-subcategories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Cheese Subcategories",
    "text": "Price Changes for Cheese Subcategories"
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-frozen-food-subcategories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-frozen-food-subcategories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Frozen Food Subcategories",
    "text": "Price Changes for Frozen Food Subcategories"
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-food-cupboard-subcategories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-food-cupboard-subcategories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Food Cupboard Subcategories",
    "text": "Price Changes for Food Cupboard Subcategories"
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-drinks-subcategories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-drinks-subcategories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Drinks Subcategories",
    "text": "Price Changes for Drinks Subcategories"
  },
  {
    "objectID": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-bakery-subcategories",
    "href": "posts/2022-09-20-tesco-price-analysis.html#price-changes-for-bakery-subcategories",
    "title": "Looking into food price inflation using data from Tesco",
    "section": "Price Changes for Bakery Subcategories",
    "text": "Price Changes for Bakery Subcategories"
  },
  {
    "objectID": "posts/2022-10-21-spotify.html",
    "href": "posts/2022-10-21-spotify.html",
    "title": "A deep dive into Spotify audio features",
    "section": "",
    "text": "Source: https://techcrunch.com/2020/06/29/in-a-significant-expansion-spotify-to-launch-real-time-lyrics-in-26-markets/\nI’ve been a user of Spotify now since 2017 and during that time I’ve created a myriad of playlists, most of which I’m willing to admit I haven’t touched in a long time. Generally speaking, my playlists fall into one of four categories.\nFirst, and most predominantly, is based on time. I create bimonthly playlists in which I add songs I’ve enjoyed throughout the two months, at the end of the year I’ll compile all 6 of those into a yearly round-up playlist. Second is based on music genre, I have playlists for each genre I listen to (of course there’s bound to be a bit of overlap). Third is based on language, I listen to a mixture of English, Japanese, Korean, and Cantonese songs. Fourth, and finally, are playlists created from song radios or other Spotify-generated means.\nOne thing that you might not know about Spotify is that it has an API that can be used by developers to create Spotify apps. It’s called the Spotify Web API and it allows you to control audio playback, manage your Spotify library, get metadata of tracks, artists, and albums, and so much more. I’m utilising the Spotipy Python library to use it. For my purposes I’ll be using it to get my playlists’ metadata and the audio features of tracks.\nIn this blog we’ll be going on a journey that explores my playlists in a data-driven way and eventually produce a machine learning algorithm that gives the most similar tracks in my playlists when provided with a track. All plots, where possible, will be made using Plotly (which is natively interactively) so hover over them, click on them, and drag around on them - see what happens!"
  },
  {
    "objectID": "posts/2022-10-21-spotify.html#data-dictionary",
    "href": "posts/2022-10-21-spotify.html#data-dictionary",
    "title": "A deep dive into Spotify audio features",
    "section": "Data Dictionary",
    "text": "Data Dictionary\nYou might be looking at some of those column names with no idea what they mean or represent, luckily Spotify does provide explanations for their audio features. And even luckier for you, I’ve created a data dictionary that describes what each column represents. Note, the language column was created used my language playlists and the genre column was created using data from Every Noise at Once.\n\n\n\n\n\n\n\n\n\nColumn\nCategory\nDescription\n\n\n\n\nname\nTrack Property\nName of the track.\n\n\nartist\nTrack Property\nName of the artist.\n\n\npopularity\nArtist Property\nThe popularity of the artist. The value will be between 0 and 100, with 100 being the most popular. The artist’s popularity is calculated from the popularity of all the artist’s tracks.\n\n\nplaylist_name\nTrack Property\nName of playlist in which this track resides.\n\n\nplaylist_date_added\nTrack Property\nDate and time when the track was added to playlist.\n\n\ndanceability\nMood\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\n\n\nenergy\nMood\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nloudness\nTrack Property\nThe overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values typically range between -60 and 0 db.\n\n\nspeechiness\nTrack Property\nSpeechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words.\n\n\nacousticness\nContext\nA confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.\n\n\ninstrumentalness\nTrack Property\nPredicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content.\n\n\nliveness\nContext\nDetects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.\n\n\nvalence\nMood\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\ntempo\nMood\nThe overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.\n\n\nlang_jap\nLanguage\nWhether the track features Japanese language, binary value of 0 or 1.\n\n\nlang_kor\nLanguage\nWhether the track features Korean language, binary value of 0 or 1.\n\n\nlang_can\nLanguage\nWhether the track features Cantonese language, binary value of 0 or 1.\n\n\nlang_eng\nLanguage\nIf a track doesn’t feature Japanese, Korean, or Cantonese it’s assumed to be English. Binary value of 0 or 1.\n\n\npop\nGenre\nBinary value that describes whether a track’s artist is labelled as pop.\n\n\nrock\nGenre\nBinary value that describes whether a track’s artist is labelled as rock.\n\n\nhip_hop\nGenre\nBinary value that describes whether a track’s artist is labelled as hip-hop.\n\n\nindie\nGenre\nBinary value that describes whether a track’s artist is labelled as indie.\n\n\nrap\nGenre\nBinary value that describes whether a track’s artist is labelled as rap.\n\n\nalternative\nGenre\nBinary value that describes whether a track’s artist is labelled as alternative."
  },
  {
    "objectID": "posts/2022-10-21-spotify.html#starting-simple-with-univariate-analysis",
    "href": "posts/2022-10-21-spotify.html#starting-simple-with-univariate-analysis",
    "title": "A deep dive into Spotify audio features",
    "section": "Starting Simple with Univariate Analysis",
    "text": "Starting Simple with Univariate Analysis\nIt’s always good to start with simple descriptive analysis to get a feel for the data. Let’s first start by looking at how many tracks are in each playlist.\n\n\n\n                                                \n\n\nClearly 2019 was a good year for music with 973 tracks being added to the ‘2019 Complete Round Up’ throughout the year. That’s 2.7 tracks per day on average! I’m quite picky with the tracks I’ll add to my bimonthly playists, I’d give an estimate of 1 track being added for every 15 tracks listened to - going by that I was listening to about 41 new tracks every day. For some context, 2019 was when I was finishing my master’s degree in Data Science during which I was listening to music while studying.\nLet’s take a look at the distribution of the features in the dataset.\n\nDanceabilityEnergyValenceTempoLoudnessDurationArtist PopularityGenreLanguage\n\n\nDanceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.\nThe median value (value in the middle when sorted in ascending order) is 0.636 and the middle half of the data (quartile 1 to 3) is between 0.515 and 0.726. Going by the above definition of danceability given by Spotify, I’m inclined to say that 75% (quartile 2, 3, and 4) of my tracks are pretty danceable! The distribution also looks to have a bit of a left skew.\n\n\n\n                                                \n\n\n\n\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy.\nThe median value is 0.638 and the middle half of the data (quartile 1 to 3) is between 0.488 and 0.791. If you were to pick a random song from all of the above playlists pooled together, there’s a good chance it’s got a decent level of energy (decent being defined as 0.5 and above). Interestingly, the distribution seems as if it’s been right censored due to the upper bound of 1.\n\n\n\n                                                \n\n\n\n\nValence is a measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. High valence means more positive sounding (happy, cheerful, etc.) while low valence sounds more negative (sad, angry, etc.)\nThis is an interesting distribution, the median lies almost exactly in the middle with a value of 0.4995. The middle half of the data has lies between 0.333 and 0.687 which pretty much covers the middle third of the bounds. In essence, this is telling me that half my tracks are neutral, one quarter are sad/angry/depressed sounding (have a value of below 0.333), and one quarter are happy/cheerful/euphoric sounding (have a value of above 0.687).\n\n\n\n                                                \n\n\n\n\nThis feature represents the overall estimated tempo of a track in beats per minute (BPM).\nThe median BPM of tracks is about 120 BPM with 50% of tracks being between 98 and 140 BPM. This almost exactly aligns with an article from MasterClass that states: “Most of today’s popular songs are written in a tempo range of 100 to 140 BPM”.\nHaven’t got too much to say about this, it makes sense that there will be a lower-bound BPM threshold that most tracks will be above and conversely an uppder-bound threshold that most tracks will be below. I’d imagine this distribution would drastically change if someone were to exclusively listen to EDM, Techno, or similar high-tempo genres.\n\n\n\n                                                \n\n\n\n\nThe overall loudness of a track in decibels (dB). This looks like a textbook example of a left (or negative) skewed distribution. I don’t know enough about decibels in the context of music mixing and mastering (or any context for that matter) to meaningfully comment on this.\n\n\n\n                                                \n\n\n\n\nHere we’ve got the distribution of track durations in seconds. This looks like a normal-ish distribution with a long tail on the right (we can test to see how close it is to a normal distribution using a Q-Q plot but I’ll skip that here).\nWhat we can get from this is that the median track length is 3 minutes and 45 seconds and half of the tracks have a duration between 3 minutes and 11 seconds and 4 minutes and 19 seconds. I’ve got one track on the very far right that has a duration of 13 minutes and 44 seconds.\n\n\n\n                                                \n\n\n\n\nSpotify has a measure for the populary of an artist which will be between 0 and 100, with 100 being the most popular. The artist’s popularity is calculated from the popularity of all the artist’s tracks.\nI’ve de-duplicated the data based on artists so we’ll have one row/track per artist to see the distribution of artist popularity. Not-so-surprisingly about 33% of the artists I listen to have a popularity value of 0. Those look to be artists/bands with small followings.\n\n\n\n                                                \n\n\n\nartist_df = df.drop_duplicates(subset='artist')\nartist_df[artist_df['popularity']==0][['lang_jap','lang_kor','lang_can','lang_eng', 'indie']].sum(axis=0)\n\nlang_jap    137\nlang_kor     74\nlang_can      3\nlang_eng    383\nindie       144\ndtype: int64\n\n\n\n\nGenre is a bit of a funny one since it’s not representing the genre(s) of the track, but rather the genre(s) of the track’s artist. While this isn’t perfect, it does act as a good enough proxy for our purposes. Note that a track’s artist can be assigned more than one genre.\nAt first glance it does seem like Pop is the most common genre but I think it needs to be remembered that Pop is probably the most paired/combined genre (e.g. Pop Rock or Indie Pop).\n\n\n\n                                                \n\n\n\n\nUnfortunately Spotify’s API doesn’t provide any metadata on the language of a track. Luckily I have language based playlists, and from that I can say any tracks that aren’t in those playlists are English tracks. That probably works for let’s say 85-90% of them."
  },
  {
    "objectID": "posts/2022-10-21-spotify.html#moving-to-two-dimensions-with-bivariate-analysis",
    "href": "posts/2022-10-21-spotify.html#moving-to-two-dimensions-with-bivariate-analysis",
    "title": "A deep dive into Spotify audio features",
    "section": "Moving to Two Dimensions with Bivariate Analysis",
    "text": "Moving to Two Dimensions with Bivariate Analysis\n\nCorrelation Analysis\nWhile we managed to learn about the features individually in the analysis above, we didn’t touch on the relationship between two features (if there is even a relationship). One way we can compactly visualise whether pairings of features have a relationship is to construct an N×N correlation matrix where N is the number of features.\nThere are several different measures that can be used to calculate the correlation between two variables, the de facto standard is Pearson’s correlation coefficient which measures the strength and direction of the linear relationship between two continuous variables. Another measure is Spearman’s rank correlation coefficient which is a nonparametric measure of rank correlation; in other words, it measures the correlation between the rankings of two variables (a good visual example of this is on its Wikipedia page). Unlike Pearson’s r, Spearmans’ ρ can be used to assess monotonic relationships (linear relationships are a subset of these) for both continuous and discrete ordinal (ranked order) variables.\nVery recently, in March 2019, a new measure of correlation was unveiled that is able both capture non-linear dependencies and work consistently between categorical, ordinal, and continuous variables. It is named 𝜙K (PhiK) and the technical details, alongside a few practical examples, are found in its paper. Unlike both Pearson’s r and Spearman’s ρ, 𝜙K is bounded between 0 and 1 which tells us the strength but not the direction of a relationship. This is because direction isn’t well-defined for a non-monotonic relationship, as you’ll see below.\nI’ve created a graph below to compare the three correlation coefficients on synthetic datasets (which is based on the first image from Wikipedia’s page on Correlation). As you can see, the third and fourth row of charts are all non-linear relationships between x and y which, as expected, result in a correlation coefficient value of zero for both Pearson’s r and Spearman’s ρ. On the other hand, 𝜙K is able to identify these relationships and the values produced are statistically significant at the 0.1% level. Exciting stuff!\n\n\n\n\n\n\nP-values are in the brackets next to the calculated correlation coefficient values, this is used to determine whether the values are statistically significant or not at a pre-defined significance level.\n\n\n\n\n\n\n\n\n\n\n\n\nThe ρ in ‘True ρ’ in the first row of charts represents the population Pearson correlation coefficient and not Spearman’s ρ.\n\n\n\nFor a sample of data from the population, r is used instead of ρ and is referred to as the sample Pearson correlation coefficient.\n\n\n\n\n\n                                                \n\n\nArmed with these tools, let’s go ahead and use them to create correlation matrices for the Spotify audio features.\n\nPearson’s rSpearman’s ρ𝜙K\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\n\n\n                                                \n\n\n\n\n\nThere’s not much of a difference when comparing the Pearson and Spearman correlation matrices but looking at the 𝜙K matrix unveils a non-linear relationship between tempo and danceability that wasn’t captured by the first two measures. I’ve made a table below that pick out pairs of features with an absolute correlation value of 0.5 or above for any of the three measures.\n\n\n\n\n\n\n  \n    \n      \n      Feature 1\n      Feature 2\n      PhiK\n      Pearson\n      Spearman\n    \n  \n  \n    \n      1\n      energy\n      loudness\n      0.80\n      0.78\n      0.77\n    \n    \n      2\n      energy\n      acousticness\n      0.67\n      -0.62\n      -0.62\n    \n    \n      3\n      loudness\n      acousticness\n      0.56\n      -0.52\n      -0.50\n    \n    \n      0\n      danceability\n      tempo\n      0.51\n      -0.22\n      -0.16\n    \n  \n\n\n\n\nA thing to keep in mind is that these are summary statistics, to get the full picture we need to take a look at the scatterplot of feature pairs.\n\nEnergy vs LoudnessEnergy vs AcousticnessLoudness vs AcousticnessDanceability vs Tempo\n\n\nThere’s a strong positive and almost linear correlation between energy and loudness which makes sense because energy measures the intensity of a track and loud tracks can be thought to be of as more intense. Note that energy is a measure that’s calculated by Spotify so there’s a good chance that it’s a function of loudness and some other track properties.\n\n\n\n                                                \n\n\n\n\nThere seems to be a moderate negative correlation between the energy of a track and the confidence of whether a track is acoustic. Acoustic music is music that mainly uses unamplified instruments that produce sound only by acoustic means. This means no eletric or virtual instruments. As you can imagine, acoustic tracks would have a lower energy compared to tracks that utilise electric or electronic instruments, so this correlation makes sense to me.\n\n\n\n                                                \n\n\n\n\nSimilar logic for the correlation between energy and acousticness applies here.\n\n\n\n                                                \n\n\n\n\nThis one is an interesting one. Going by the LOWESS (Locally Weighted Scatterplot Smoothing) trendline, there seems to be sweet-spot in tempo where the most danceable tracks are. Going back to the definition of danceability, it’s calculated based on a combination of tempo, rhythm stability, beat strength, and overall regularity. We know that there’s a definitive relationship between the two variables, and from the below chart we can clearly see it’s a non-linear relationship.\n\n\n\n                                                \n\n\n\n\n\n\n\nAnalysis by Time, Playlist, and Genre\nLet’s now analyse the how Spotify audio features change throughout time, how they vary by playlists (only considering the major ones), and whether different genres can be distinguished solely through these features.\n\n\nYearly PlaylistsLanguage and Genre PlaylistsTrack Genre\n\n\nHere we’re looking at how my music taste changes over the years. It’s immediately clear 2022 (up until August) has included more energetic and valent tracks (what I’d like to call ‘positive vibes’) marked by the median values and tighter inter-quartile range.\nI’ve plotted the boxplots with a notch representing the 95% confience interval of the median which offers a rough guide of the significance of the difference of medians; if the notches of two boxes do not overlap, this will provide evidence of a statistically significant difference between the medians. Thus, we can say that there is a statistically significant difference in medians at the 5% level in the medians between 2021 and 2022 for the energy and valence features.\n\n\n\n                                                \n\n\n\n\nTEST\n\n\n\n                                                \n\n\n\n\nTEST\n\n\n\n                                                \n\n\n\n\n\n\n\nfig = px.scatter_3d(\n    df.drop_duplicates(subset=['id']),\n    x='danceability',\n    y='energy',\n    z='valence',\n    color='tempo',\n    width=800,\n    height=800\n)\n\nfig.update_traces(\n    marker=dict(size=2),\n    selector=dict(mode='markers')\n)\n\nfig.show()\n\n\n                                                \n\n\n\n# Initalise array of zeros to store results in\nscore_arr = np.zeros((len(genres), len(genres)))\n\n# Calculate Jaccard similarity score for pairs of genres of tracks\n# Tells us the proportion of co-occurance of genres for tracks are classified either or both genres\nfor idx1, genre1 in enumerate(genres):\n    for idx2, genre2 in enumerate(genres):\n        score_arr[idx1][idx2] = jaccard_score(df[genre1] == 1, df[genre2] == 1)\n\n# Plot results\nplt.figure(figsize=(12,10))\nsns.heatmap(\n    data=score_arr,\n    cmap='Blues',\n    robust=True,\n    square=True,\n    linewidths=0.5,\n    annot=True,\n    fmt='.2f',\n    annot_kws={'fontsize': 12},\n    cbar=True,\n    cbar_kws={'shrink': 0.95},\n    xticklabels=[' '.join(g.split('_')).title() for g in genres],\n    yticklabels=[' '.join(g.split('_')).title() for g in genres],\n)\nplt.title('Jaccard similarity coefficient of genres', fontsize=18)\nplt.xlabel('Genre', fontsize=15)\nplt.ylabel('Genre', fontsize=15)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.tight_layout()\n\nsave_path = f'{FIG_SAVE_PATH}/jaccard_similary_coeffs_genres.png'\nif not path.exists(save_path):\n    plt.savefig(save_path, bbox_inches='tight')\n\nplt.show()\n\n\n\n\n\n# Let's find out 'correlation' between combined genres (e.g. tracks that are assigned both pop and rock genres) with single genres (e.g. indie)\n\n# First is to get the combined genres we're interested in\n# We don't want permutations, i.e. we treat pop & rock and rock & pop as the same thing\n# We also want to only get combined genres that have a similarity score of > 0.1 for the two individual genres\nrec_set = set()\nfor genre1 in genres:\n    for genre2 in genres:\n        if genre1 == genre2:\n            continue\n        elif (f\"{genre1},{genre2}\" in rec_set) or (f\"{genre2},{genre1}\" in rec_set):\n            continue\n        else:\n            if jaccard_score(df[genre1] == 1, df[genre2] == 1) < 0.1:\n                continue\n            else:\n                rec_set.add(f\"{genre1},{genre2}\")\n\n# Initalise array of zeros to store results in \nscore_arr = np.zeros((len(rec_set), len(genres)))\n\n\n# Calculate Jaccard similarity score for combined genres and individual genres of tracks\nfor idx1, comb_genre in enumerate(list(rec_set)):\n    print(comb_genre)\n    comb_genre1, comb_genre2 = comb_genre.split(',')\n    for idx2, genre in enumerate(genres):\n        score_arr[idx1][idx2] = jaccard_score(((df[comb_genre1] == 1) & (df[comb_genre2] == 1)), df[genre] == 1)\n\n# Plot results\nplt.figure(figsize=(12,10))\nsns.heatmap(\n    data=score_arr.T,\n    cmap='Blues',\n    robust=True,\n    square=True,\n    linewidths=0.5,\n    annot=True,\n    fmt='.2f',\n    annot_kws={'fontsize': 12},\n    cbar=True,\n    cbar_kws={'shrink': 0.95},\n    xticklabels=[' '.join(' & '.join(g.split(',')).split('_')).title() for g in list(rec_set)],\n    yticklabels=[' '.join(g.split('_')).title() for g in genres],\n)\nplt.title('Jaccard similarity coefficient of combined genres with single genres', fontsize=17)\nplt.xlabel('Genre', fontsize=15)\nplt.ylabel('Genre', fontsize=15)\nplt.xticks(fontsize=13,rotation=15)\nplt.yticks(fontsize=13)\nplt.tight_layout()\n\nsave_path = f'{FIG_SAVE_PATH}/jaccard_similary_coeffs_combined_genres.png'\nif not path.exists(save_path):\n    plt.savefig(save_path, bbox_inches='tight')\n\nplt.show()\n\npop,indie\nrock,indie\nhip_hop,rap\nrock,alternative\nindie,alternative\npop,rock\n\n\n\n\n\n\nnn_feat_cols = [\n    'danceability',\n    'energy',\n    'loudness',\n    # 'speechiness',\n    'acousticness',\n    # 'instrumentalness',\n    # 'liveness',\n    'valence',\n    'tempo',\n    # 'key',\n    # 'time_signature',\n    'lang_eng',\n    'lang_kor',\n    'lang_jap',\n    'lang_can',\n    'pop', \n    'rock', \n    'hip_hop', \n    'indie', \n    'rap',\n    'alternative',\n]\n\n# Manually set weightings on columns to bias them, higher weights puts higher bias on a feature\nWEIGHTING = False\nweighting = {\n    'danceability': 1,\n    'energy': 1,\n    'loudness': 1,\n    'valence': 1,\n    'instrumentalness': 1,\n    'acousticness': 1,\n    'tempo': 1,\n    'key': 1,\n    'time_signature': 1,\n    'lang_eng': 1,\n    'lang_kor': 1,\n    'lang_jap': 1,\n    'lang_can': 1,\n    'pop': 1,\n    'rock': 1,\n    'hip_hop': 1,\n    'indie': 1,\n    'rap': 1,\n    'alternative': 1,\n}\n\n## Remove duplicate songs in a way that retains the most information in terms of playlists the songs belong to\n# Mask for genre or language playlists\nmask = (df['playlist_name'].isin([name for name in df['playlist_name'].unique() if ('Complete' not in name) and ('&' not in name) and ('playlist' not in name)]))\n\n# Separate dataframe into one dataframe for playlists which are time-based (i.e. yearly or bi-monthly) and another dataframe for playlists which are genre or language based\n# Drop duplicate songs in both dataframes\ndupe_cols = [col for col in df.columns if col not in ['playlist_name', 'playlist_date_added', 'playlist_track_id']]\ndedup_lang_genre_df = df[mask].sort_values(by=['playlist_name']).drop_duplicates(subset=dupe_cols, keep='first')\ndedupe_time_df = df[~mask].sort_values(by=['playlist_name']).drop_duplicates(subset=dupe_cols, keep='first')\n\n# Concatenate both into one dataframe with the genre/lang dataframe being at the top\n# Then do a final drop duplicates so we only keep duplicate songs from genre/lang dataframe\ndedupe_df = pd.concat([dedup_lang_genre_df, dedupe_time_df]).drop_duplicates(subset=dupe_cols, keep='first')\ndf_weighted = dedupe_df.copy().drop_duplicates(subset=nn_feat_cols).drop_duplicates(subset=['name']).reset_index(drop=True)\n\n# Scale features\nfor col in ['tempo', 'loudness', 'time_signature', 'key']:\n    if col in nn_feat_cols:\n        df_weighted[col] = minmax_scale(df_weighted[col].values)\n\n# Weight features if required\nif WEIGHTING == True:\n    for col in nn_feat_cols:\n        df_weighted[col] = weighting[col] * np.abs(df_weighted[col])\n\n\ndf_weighted['playlist_name'].value_counts()\n\nJapanese                           861\nIndie Alt                          685\n2018 Complete Round Up             297\nKorean                             272\n2017 Complete Round Up             270\nChill                              234\nPop                                234\nRap/HipHop/Trap                    205\n2021 Complete Round Up              86\n2019 Complete Round Up              83\nCantonese                           78\n2020 Complete Round Up              66\nCrème de la Crème                   38\n2022 Complete Round Up              38\nMood/Instrumental                   30\nJanuary & February 2020             23\nNovember & December 2019            16\nUpbeat                              14\nMay & June 2019                     10\nAugust & September 2021             10\nMarch & April 2019                  10\nSeptember & October 2019             9\nJanuary & February 2022              9\nJuly & August 2019                   9\nJanuary & February 2019              9\nMay & June 2022                      5\nSpecial Select J-indie playlist      3\nMarch & April 2020                   1\nName: playlist_name, dtype: int64\n\n\n\ncol_lists = [\n    [\n        'danceability',\n        'energy',\n        'loudness',\n        'acousticness',\n        'valence',\n        'tempo'\n    ],\n    [\n        'lang_eng',\n        'lang_kor',\n        'lang_jap',\n        'lang_can'\n    ],\n    [\n        'pop', \n        'rock', \n        'hip_hop', \n        'indie', \n        'rap',\n        'alternative'\n    ],\n]\ntitle_map = {\n    0: 'UMAP embedding created using audio track features',\n    1: 'UMAP embedding created using audio track features and language indicator',\n    2: 'UMAP embedding created using audio track features and language and genre indicators',\n}\n\nfor idx, lst in enumerate(col_lists):\n    cols = [item for lst in col_lists[:idx+1] for item in lst]\n    \n    # DR using UMAP for 2d vis of data\n    reducer = umap.UMAP(n_neighbors=6, random_state=42)\n    embedding = reducer.fit_transform(df_weighted[cols])\n\n    embedding_df = pd.DataFrame(embedding, columns=['dim_1', 'dim_2']).join(df_weighted[['playlist_name']])\n\n    fig, ax = plt.subplots(figsize=(18,12))\n\n    mask = embedding_df['playlist_name'].isin(['Korean', 'Japanese', 'Cantonese', 'Indie Alt', 'Pop', 'Rap/HipHop/Trap', 'Chill', 'Mood/Instrumental', 'Upbeat'])\n    \n    sns.scatterplot(\n        x=embedding_df[mask]['dim_1'],\n        y=embedding_df[mask]['dim_2'],\n        s=50,\n        alpha=0.9,\n        ax=ax,\n        hue=embedding_df[mask]['playlist_name']\n    )\n\n    plt.xlabel('Embedding dimension 1', fontsize=15)\n    plt.ylabel('Embedding dimension 2', fontsize=15)\n    plt.title(f'{title_map[idx]}', fontsize=18)\n    plt.legend(title='Playlist name', title_fontsize=15, fontsize=14, loc='lower right')\n    plt.tight_layout()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n# DR using UMAP for 2d vis of data\nreducer = umap.UMAP(n_neighbors=6, random_state=234)\nembedding = reducer.fit_transform(df_weighted[nn_feat_cols])\n\nembedding_df = pd.DataFrame(embedding, columns=['dim_1', 'dim_2']).join(df_weighted[['playlist_name','lang_jap','lang_kor','lang_can','lang_eng','pop','rock','hip_hop','indie','rap','rnb','alternative']])\n\n\ntitle_map = {\n    'lang_jap': 'Japanese',\n    'lang_kor': 'Korean',\n    'lang_can': 'Cantonese',\n    'lang_eng': 'English',\n    'pop': 'Pop',\n    'rock': 'Rock', \n    'hip_hop': 'Hip Hop',\n    'indie': 'Indie',\n    'rap': 'Rap',\n    'alternative': 'Alternative',\n}\n\nplt.figure(figsize=(35,25))\n\nfor idx, genre in enumerate(title_map, 1):\n    plt.subplot(3,4,idx)\n    sns.scatterplot(\n        x=embedding_df['dim_1'],\n        y=embedding_df['dim_2'],\n        s=40,\n        alpha=0.5,\n        hue=embedding_df[genre]\n    )\n    plt.xlabel('Embedding Dimension 1', fontsize=13)\n    plt.ylabel('Embedding Dimension 2', fontsize=13)\n    plt.title(f'UMAP embedding by {title_map[genre]} attribute', fontsize=15)\n    plt.legend(title=title_map[genre], title_fontsize=13, fontsize=12, loc='upper left')\n\nplt.suptitle('Track attributes of UMAP embedding projection', fontsize=19)\nplt.tight_layout(rect=[0, 0, 1, 0.98])\nplt.show()\n\n\n\n\n\n\n# DR using UMAP for 3d vis of data\nreducer = umap.UMAP(n_neighbors=30, n_components=3)\nembedding = reducer.fit_transform(df_weighted[nn_feat_cols])\n\nfig = px.scatter_3d(\n    x=embedding[:, 0],\n    y=embedding[:, 1],\n    z=embedding[:, 2],\n    width=800,\n    height=800\n)\n\nfig.update_traces(\n    marker=dict(size=2),\n    selector=dict(mode='markers')\n)\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  }
]